#!/usr/bin/env python3
"""
åˆ›å»ºæ›´çœŸå®çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†
"""

import json
import random
from typing import List, Dict


class BenchmarkDatasetGenerator:
    def __init__(self):
        # ä¸åŒç±»å‹å’Œé•¿åº¦çš„æµ‹è¯•ç”¨ä¾‹
        self.short_questions = [
            "1+1ç­‰äºå¤šå°‘ï¼Ÿ",
            "ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ",
            "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ",
            "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯AIï¼Ÿ",
        ]

        self.medium_questions = [
            "è¯·è§£é‡Šæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒºåˆ«ï¼Œå¹¶ä¸¾ä¾‹è¯´æ˜å®ƒä»¬çš„åº”ç”¨åœºæ™¯ã€‚",
            "å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜å¹¶å‘çš„WebæœåŠ¡æ¶æ„ï¼Ÿè¯·è¯¦ç»†è¯´æ˜å„ä¸ªç»„ä»¶çš„ä½œç”¨ã€‚",
            "åˆ†æå½“å‰äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•è¶‹åŠ¿ï¼Œä»¥åŠå¯¹å„è¡Œä¸šå¯èƒ½äº§ç”Ÿçš„å½±å“ã€‚",
            "è¯·ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°æ¥å®ç°äºŒåˆ†æœç´¢ç®—æ³•ï¼Œå¹¶åˆ†æå…¶æ—¶é—´å¤æ‚åº¦ã€‚",
        ]

        self.long_questions = [
            """è¯·è¯¦ç»†åˆ†æä»¥ä¸‹ä»£ç çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶æä¾›ä¼˜åŒ–å»ºè®®ï¼š

def process_data(data_list):
    result = []
    for item in data_list:
        if item > 0:
            for i in range(len(data_list)):
                if data_list[i] == item:
                    result.append(item * 2)
    return result

åŒæ—¶è¯·è§£é‡Šä¸ºä»€ä¹ˆè¿™æ ·çš„ä¼˜åŒ–èƒ½å¤Ÿæå‡æ€§èƒ½ï¼Œä»¥åŠåœ¨ä»€ä¹ˆæƒ…å†µä¸‹è¿™ç§ä¼˜åŒ–æœ€æœ‰æ•ˆã€‚""",
            """åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡ä¸­ï¼ŒCAPå®šç†æ˜¯ä¸€ä¸ªé‡è¦çš„æ¦‚å¿µã€‚è¯·è¯¦ç»†è§£é‡Šï¼š
1. CAPå®šç†çš„å…·ä½“å†…å®¹å’Œå«ä¹‰
2. ä¸ºä»€ä¹ˆä¸èƒ½åŒæ—¶æ»¡è¶³æ‰€æœ‰ä¸‰ä¸ªç‰¹æ€§
3. åœ¨å®é™…ç³»ç»Ÿè®¾è®¡ä¸­å¦‚ä½•æ ¹æ®ä¸šåŠ¡éœ€æ±‚è¿›è¡Œå–èˆ
4. ä¸¾ä¾‹è¯´æ˜ä¸åŒç±»å‹çš„åˆ†å¸ƒå¼ç³»ç»Ÿæ˜¯å¦‚ä½•å¤„ç†CAPæƒè¡¡çš„
5. ç°ä»£åˆ†å¸ƒå¼ç³»ç»Ÿå¦‚ä½•é€šè¿‡æŠ€æœ¯æ‰‹æ®µå°½å¯èƒ½åœ°å¹³è¡¡è¿™ä¸‰ä¸ªç‰¹æ€§

è¯·ç»“åˆå…·ä½“çš„æŠ€æœ¯å®ç°å’ŒçœŸå®æ¡ˆä¾‹æ¥è¯´æ˜ã€‚""",
        ]

    def generate_dataset(
        self,
        total_samples: int = 300,
        short_ratio: float = 0.3,
        medium_ratio: float = 0.5,
        long_ratio: float = 0.2,
    ) -> List[Dict]:
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æ•°æ®é›†"""

        dataset = []

        # è®¡ç®—å„ç±»å‹é—®é¢˜çš„æ•°é‡
        short_count = int(total_samples * short_ratio)
        medium_count = int(total_samples * medium_ratio)
        long_count = total_samples - short_count - medium_count

        # ç”ŸæˆçŸ­é—®é¢˜
        for _ in range(short_count):
            question = random.choice(self.short_questions)
            dataset.append(self._create_conversation(question))

        # ç”Ÿæˆä¸­ç­‰é•¿åº¦é—®é¢˜
        for _ in range(medium_count):
            question = random.choice(self.medium_questions)
            dataset.append(self._create_conversation(question))

        # ç”Ÿæˆé•¿é—®é¢˜
        for _ in range(long_count):
            question = random.choice(self.long_questions)
            dataset.append(self._create_conversation(question))

        # æ‰“ä¹±é¡ºåº
        random.shuffle(dataset)

        return dataset

    def _create_conversation(self, question: str) -> Dict:
        """åˆ›å»ºå¯¹è¯æ ¼å¼"""
        return {
            "conversations": [
                {"from": "human", "value": question},
                {"from": "gpt", "value": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å›ç­”ã€‚"},  # å ä½ç¬¦
            ]
        }

    def save_dataset(self, dataset: List[Dict], filename: str):
        """ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶"""
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)

        print(f"âœ… æ•°æ®é›†å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š æ€»è®¡ {len(dataset)} æ¡è®°å½•")


def main():
    generator = BenchmarkDatasetGenerator()

    # ç”Ÿæˆä¸åŒè§„æ¨¡çš„æ•°æ®é›†
    datasets = [
        # (100, "small_benchmark_dataset.json"),
        # (500, "medium_benchmark_dataset.json"),
        (1000, "large_benchmark_dataset.json")
    ]

    for size, filename in datasets:
        print(f"\nğŸ”„ ç”Ÿæˆ {filename}...")
        dataset = generator.generate_dataset(total_samples=size)
        generator.save_dataset(dataset, filename)


if __name__ == "__main__":
    main()
